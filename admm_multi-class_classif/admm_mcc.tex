%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Short Sectioned Assignment
% LaTeX Template
% Version 1.0 (5/5/12)
%
% This template has been downloaded from:
% http://www.LaTeXTemplates.com
%
% Original author:
% Frits Wenneker (http://www.howtotex.com)
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[paper=a4, fontsize=11pt]{scrartcl} % A4 paper and 11pt font size

\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{pifont}
\usepackage{hyperref}
\usepackage[T1]{fontenc} % Use 8-bit encoding that has 256 glyphs
\usepackage{graphicx} % Required for including images
%\usepackage{fourier} % Use the Adobe Utopia font for the document - comment this line to return to the LaTeX default
\usepackage[english]{babel} % English language/hyphenation
\usepackage{amsmath,amsfonts,amsthm} % Math packages

\usepackage{lipsum} % Used for inserting dummy 'Lorem ipsum' text into the template

\usepackage{sectsty} % Allows customizing section commands
\allsectionsfont{\centering \normalfont\scshape} % Make all sections centered, the default font and small caps

\usepackage{fancyhdr} % Custom headers and footers
\pagestyle{fancyplain} % Makes all pages in the document conform to the custom headers and footers
\fancyhead{} % No page header - if you want one, create it in the same way as the footers below
\fancyfoot[L]{} % Empty left footer
\fancyfoot[C]{} % Empty center footer
\fancyfoot[R]{\thepage} % Page numbering for right footer
\renewcommand{\headrulewidth}{0pt} % Remove header underlines
\renewcommand{\footrulewidth}{0pt} % Remove footer underlines
\setlength{\headheight}{13.6pt} % Customize the height of the header

\numberwithin{equation}{section} % Number equations within sections (i.e. 1.1, 1.2, 2.1, 2.2 instead of 1, 2, 3, 4)
\numberwithin{figure}{section} % Number figures within sections (i.e. 1.1, 1.2, 2.1, 2.2 instead of 1, 2, 3, 4)
\numberwithin{table}{section} % Number tables within sections (i.e. 1.1, 1.2, 2.1, 2.2 instead of 1, 2, 3, 4)

\setlength\parindent{0pt} % Removes all indentation from paragraphs - comment this line for an assignment with lots of text

%----------------------------------------------------------------------------------------
%	TITLE SECTION
%----------------------------------------------------------------------------------------

\newcommand{\horrule}[1]{\rule{\linewidth}{#1}} % Create horizontal rule command with 1 argument of height

\title{	
\normalfont \normalsize 
\textsc{WSU, CS} \\ [25pt] % Your university, school and/or department name(s)
\horrule{0.5pt} \\[0.4cm] % Thin top horizontal rule
\huge Applying ADMM for multi-class classification problem. \\ % The assignment title
\horrule{2pt} \\[0.5cm] % Thick bottom horizontal rule
}

\parindent=1cm

\author{Artem Komarichev} % Your name

\date{\normalsize\today} % Today's date or a custom date

\begin{document}

\maketitle % Print the title

%----------------------------------------------------------------------------------------
%	PROBLEM 1
%----------------------------------------------------------------------------------------


\textbf{Goal:} using logistic regression recognize handwritten digits (from 0 to 9). There is ten classes ($K=10$) \\

\textbf{Dataset:} 5000 training examples (let's name it $m$) and each sample contains 400 features (pixels in a 20x20 grid).

\section{Problem formulation} 
In the case of L2 regularized logistic regression the problem becomes:

\begin{align} 
\begin{split}
	minimize \quad \frac{1}{m} \sum_{i=1}^{m}(-y \log h_{\theta}(x)-(1-y)\log (1-h_{\theta}(x)) + \frac{\lambda}{2m} \sum \theta_j^2
\end{split}					
\end{align} 
where $h_{\theta}(x) = g(\theta^Tx)$ and $g(z)=\frac{1}{1+e^{-z}}$ - sigmoid function. \\

The ADMM algorithm for the above problem is $[1]$:

\begin{align} 
\begin{split}
	& x^{k+1} = \arg\min_{x} \frac{1}{m} \sum_{i=1}^{m}l(x) + \frac{\lambda}{2m} \sum ||x-z^k+u^k||_2^2 \\
	& z^{k+1} = S_{\frac{m\lambda}{\rho}}(x^{k+1}+u^k) \\
	& u^{k+1} = u^k + x^{k+1} - z^{k+1}
\end{split}					
\end{align} 
where $l(x) = -y \log h_{\theta}(x)-(1-y)\log (1-h_{\theta}(x)$.\\

The x-update is a proximal operator evaluation. And this was solved by using quasi-Newton method (L-BFGS). Correspondingly, the partial derivatives of regularized logistic regression cost for $x$ defined as in $[2]$: 
\begin{align} 
\begin{split}
	& \frac{1}{m} \sum_{i=1}^{m}[x^T(h_{\theta}(x)-y) + \lambda (x-z^k+u^k)] \\
\end{split}					
\end{align}
\\

\textbf{Drawback}: As you can notice second step in (1.2) is the soft thresholding operator. We can use shrinkage only in the case when our regulizer is $l_1$! But I was using the $l_2$ regularization in the cost function (1.1). So I have to either replace z-update with the proximity operator or replace $l_2$ on $l_1$ in the objective function.

%\begin{center}
%\includegraphics[width=0.3\linewidth]{cd.png}
%\par\large\textit{Fig. 1. Function of two variables $u=f(x,y)$. \\ Along the lines, %the function has a constant value equal to 1, 3, 5, 7, or 9.}
%\end{center}

%------------------------------------------------

\section{One-vs-all Classification}

	I've implemented one-vs-all classification by training multiple regularized logistic regression classifiers, one for each of the $K$ classes in dataset (in our case $K=10$). \\

While training the classifire for class $k\in\{1,...,K\}$, I was creating a $m-$dimensional vector of labels $y$, where $y_j \in 0,1$ indicates whether the $j^{th}$ sample belongs to class $k$ $(y_j = 1)$, or if it belongs to a different class ($y_j=0$). \\

\section{One-vs-all Prediction}

	After the previous step (training one-vs-all classifier), I was able to predict the digit for each sample. Simply by calculating the "probability" that it belongs to each class using already trained classifier. And we should assign an appropriate digit to it based on the highest probability.  \\


\section{Conclusion}

As I did already mention in one of my previous email. The highest accuracy I got is about $97.36\%$. Authors of the assignment got the accuracy about $94.9\%$ $[2]$.

\section{References}

$[1]$ Stephen Boyd. Distributed Optimization and Statistical Learning via the Alternating Direction Method of Multipliers, 2011.\\
$[2]$ Andrew Ng. Online Machine Learning class on Coursera. Programming exercise 3: Multi-class Classification and Neural Networks, 2011. \\

\end{document}